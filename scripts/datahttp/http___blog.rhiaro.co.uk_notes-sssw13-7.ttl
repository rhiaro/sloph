@prefix dcterms: <http://purl.org/dc/terms/> .
@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .
@prefix blog: <http://vocab.amy.so/blog#> .
@prefix dc: <http://purl.org/dc/elements/1.1/> .
@prefix sioc: <http://rdfs.org/sioc/types#> .
@prefix foaf: <http://xmlns.com/foaf/0.1/> .

<http://blog.rhiaro.co.uk/notes-sssw13-7>
  dcterms:title "[Notes] Tommaso Di Noia at #SSSW2013" ;
  dcterms:created "2013-07-10T21:30:00.000Z"^^xsd:datetime ;
  dcterms:modified "2013-07-30T16:11:22.128Z"^^xsd:datetime ;
  blog:bloggerid "tag:blogger.com,1999:blog-18505529.post-554189058804958709" ;
  dc:creator <http://www.blogger.com/profile/12227954801080178130> ;
  sioc:content """**Tools and Techniques**  
**  
**_Recommender systems_  
  
Input: Set of users + set of items + rating matrix.  
Problem - given user, predict rating for an item.  
  
In real world, recommendation matrix data is sparse.  
  
Can use hybrid approaches.  
  
Collaborative RS:  
  

  * Like Amazon.
  * Based on other users with similar profiles.
  * Experimentally better than content-based, but you don't always have many users.
  
Knowledge-based RS:  
  

  * No/little user history.
  * Based on domain knowledge.
  
User-based collaborative recommendation:  
  

  * Pearson's correlation coefficient - baseline.
  * Imagine millions of users - computing similarities takes a lot of time.
  * So ..
  
Item-based collaborative recommendation:  
  

  * Focus on items not users.
  * Compute similarity between each pair of items.
  * Don't have to compute similarity between items that don't have overlapping ratings.
  * Cosine similarity / adjusted cosine similarity (taking into account average rating related to a user to eliminate some bias).
  
Content-based RS:  
  

  * Based on description of item 
  * and profile of user interests.
  
  

  * Items are described in terms of attributes/features.
  * Finite set of values associated with features.
  * Item representation is a vector.
  * Don't necessarily have complete descriptions of items - just have a 0 in your vector.
  
  

  * Similarity between items: 
    * Jaccard similarity.
    * Cosine similarity and TF-IDF (term frequency - inverse document frequency).
    * Batch compute similarities offline, then use similarities to compute ratings on the fly based on user profile.
  
  

  * Predict rate only for N nearest neighbours of items in user profile, that are not in the user profile.
  * An item is worth rating if more than x of N number of neighbours are within user profile.
  
  
_Using LOD_  
  
To mitigate lack of information/descriptions about concepts/entities.  
  
Recommender systems are usually vertical, but LD lets you easily build a
multi-domain recommender system.  
  
To avoid noisy data, you have to filter it before feeding your RS.  
  
Freebase.  
  
  
Tiapolo  
  

  * Automating typing of DBPedia entities.
  
  
Vector space model for LOD  
  

  * MATHS.
  
  
  

""" ;
  sioc:topic blog:Done, "tommaso di noia", "sssw2013", "sssw13", "semantic web tools", "semantic web summer school", "semantic web", "recommender systems", "recommendations", "phd", "notes", "linked data" ;
  foaf:isPrimaryTopicOf <http://rhiaro.co.uk/2013/07/notes-sssw13-7> .

